오늘은 웹 크롤링을 진행해볼게요.

> 웹 크롤링은 데이터를 얻을 수 있는 가장 쉬운 방법
>
> 우선 지금까지 배웠던 것들을 다시 생각해볼게요.
>
> 리스트와 셋, 딕셔너리와 같은 자료형들은 데이터들을 목적에 맞게 저장하는 것이고,
>
> 판다스는 데이터들 중 원하는 데이터만 뽑는 등 가공하는 데 사용되고,
>
> 맷플로립은 데이터를 시각화 하는데 사용되죠.
>
> 그렇다면 크롤링은, 위 내용들의 기반이 되는 데이터 그 자체를 모으는 방법 중 하나입니다.
>
> 여기도 써져있죠.

크롤링은 웹 페이지를 그대로 가져와서 데이터를 추출해내는 행위이고,

크롤러는 이를 도와주는 도구이죠.

---

파이썬에서 크롤링을 도와주는 도구는 여러가지가 있어요.

beautiful soup이나, 셀레니움과 같은 여러 도구가 있는데 이번에는 그중 가장 대표적인 beautifulsoup를 사용해볼게요.

이제부터 beautifulsoup를 줄여서 bs4라 부를게요.

bs4를 하기 전에, 우선 requests라는 모듈을 설치해야 돼요.

requests는 네트워크와 관련된 요청을 도와주는 모듈이에요.

즉, 웹사이트가 있는 서버에 http 요청을 보내는 모듈입니다.

여기서 http는 사용자와 웹 서버 사이의 약속

ex) 웹서핑하다가 볼 수 있는 `404 error` -> `만약 사용자가 웹 서버에 없는 주소에 요청을 보내면 404를 반환하자.`라는 약속

일반적으로 볼 수 없는 에러 중 하나 -> 개발자가 에러를 확인하기 위해 임의로 붙힌 코드

`http status code`를 보면, 404 말고도 다양한 코드가 있음

여기서 알 수 있는 것이 `요청`이라는 개념.

사용자가 서버에게 요청 -> 서버는 html을 반환.

google이라는 서버에 `search?q=requests`이라는 요청을 보내면,

서버는 요청에 맞는 페이지를 반환

이러한 요청은 웹 브라우저에서 사용자가...

get은 파이썬 코드로 서버에 요청을 보낼 수 있도록 해줌.

---

바로 실습을 해볼게요.

import requests로 requests 모듈을 가져오고,

url이라는 변수에 주소를 담을게요.

그리고, requests.get()의 파라미터로 해당 url을 넣을게요.

requests.get은 인자에 있는 url에 말 그대로 get 요청을 보내줘요.

그리고 response에는 get 요청의 결과가 담기죠.

정확히는 response라는 변수에 해당 서버의 응답 객체를 저장합니다. 이 객체에는 응답의 상태 코드, 헤더, 본문 등이 포함

get 요청이 정상적으로 이루어졌다면, response 200이라는 결과가 담기게 돼요. 위에서 봤던 http status code에서, `정상 작동`을 의미하는 코드.

---

만약 response 뒤에 .text를 붙여주면, get 요청의 결과로 받아온 html 코드를 텍스트로 보여줘요.

<br>

html: 페이지를 만드는 방법 중 가장 대표적인 방법.

`example.com`

에 들어가서 컨트롤 u를 누르면 html이 뜨는 것을 볼 수 있음.

여기서 알 수 있다시피, 크롬이나 firefox와 같은 웹 브라우저는 서버에서 이러한 html을 받아서, 우리가 편하게 볼 수 있도록 렌더링을 하는 프로그램이라는 것을 알 수 있음.

html은 <꺽쇠>로 이루어진 `태그`들이 조합되어 만들어지는 것을 알 수 있음.

여러분들이 잘 아는 레고가 조립되어 하나의 작품을 만드는 것처럼 html은 태그들이 조합되어 하나의 페이지를 만들게 됨.

크롤링은, 이러한 html의 특정 태그를 가져오는 것이라 이해해도 됨.

---

> pdf에 따라 쭉 진행.

`class_`: class는 기본적으로 파이썬의 예약어. `True`를 변수 명으로 사용할 수 없는 것처럼 `class`도 변수 명으로 사용할 수 없기에, bs4에서는 변수 명을 `class_`이라고 이름을 붙임.

---

### BeuatifulSoup 파트

BeautifulSoup 객체를 생성하여 resp.text에 있는 HTML 코드를 파싱합니다.

BeautifulSoup 객체를 생성할 때, 첫 번째 인자로 HTML 코드를 전달하고, 두 번째 인자로 파서(parser)를 지정합니다. 여기서는 'html.parser'를 사용합니다.

그렇게 되면, soup에는 웹사이트의 제목부터 시작해서 모든 요소들이 다 나뉘어서 들어감.

---

`print(soup.find_all('p')[0]) == print(soup.find('p'))`

---

> 정규표현식 부분에서...

날씨 웹페이지는 복잡하니 간단한 페이지로 바꿔볼게요.

> 인터넷에 news 검색해서 맨 위에 뜨는 거 가져오기

`+`: 앞에 오는 문자가 하나 이상 반복되는 것을 의미합니다.

따라서 re.compile('날씨+')는 "날씨"라는 문자열이 하나 이상 연속으로 나타나는 모든 경우를 찾습니다.

예를 들어, "날씨", "오늘날씨", "날씨 예보" 등이 모두 매칭됨.

---

할리스 매장을 구하는 예제가 있는데, 이 코드를 읽어보니까 오류가 있더라고요, 그래서, 할리스 매장 정보를 얻는 코드를 직접 같이 짜보려 합니다.

0712.ipynb 확인.
